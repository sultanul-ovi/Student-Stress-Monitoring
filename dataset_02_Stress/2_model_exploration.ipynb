{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46073c67",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb398a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFECV, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af20dae",
   "metadata": {},
   "source": [
    "## ML Model Results Storage Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a95e6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results storage framework loaded successfully!\n",
      "Available functions:\n",
      "- storeResults(model, config, accuracy, f1, recall, precision, auc_roc)\n",
      "- displayAndSaveResults(filename_prefix='model_results')\n",
      "- clearResults()\n",
      "- plotModelComparison(result_df)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ML MODEL RESULTS STORAGE FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "# Creating holders to store the model performance results\n",
    "ML_Model = []\n",
    "ML_Config = []\n",
    "accuracy = []\n",
    "f1_score = []\n",
    "recall = []\n",
    "precision = []\n",
    "auc_roc = []  # Adding a holder for AUC-ROC\n",
    "\n",
    "# Function to call for storing the results\n",
    "def storeResults(model, config, a, b, c, d, e):\n",
    "    \"\"\"\n",
    "    Store model performance results\n",
    "    \n",
    "    Parameters:\n",
    "    model: Name of the ML model\n",
    "    config: Configuration name (preprocessing steps applied)\n",
    "    a: Accuracy score\n",
    "    b: F1 score\n",
    "    c: Recall score\n",
    "    d: Precision score\n",
    "    e: AUC-ROC score\n",
    "    \"\"\"\n",
    "    ML_Model.append(model)\n",
    "    ML_Config.append(config)\n",
    "    accuracy.append(round(a, 6))\n",
    "    f1_score.append(round(b, 6))\n",
    "    recall.append(round(c, 6))\n",
    "    precision.append(round(d, 6))\n",
    "    auc_roc.append(round(e, 6))\n",
    "\n",
    "# Function to display and save results\n",
    "def displayAndSaveResults(filename_prefix='model_results'):\n",
    "    \"\"\"\n",
    "    Create dataframe from results, display, and save to CSV\n",
    "    \n",
    "    Parameters:\n",
    "    filename_prefix: Prefix for the CSV filenames\n",
    "    \"\"\"\n",
    "    # Creating the dataframe\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': [f\"{acc * 100:.3f}%\" for acc in accuracy],\n",
    "        'F1 Score': [f\"{f1 * 100:.3f}%\" for f1 in f1_score],\n",
    "        'Recall': [f\"{rec * 100:.3f}%\" for rec in recall],\n",
    "        'Precision': [f\"{prec * 100:.3f}%\" for prec in precision],\n",
    "        'ROC_AUC': [f\"{roc * 100:.3f}%\" for roc in auc_roc],\n",
    "    })\n",
    "    \n",
    "    # Remove duplicates if any\n",
    "    result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MODEL PERFORMANCE RESULTS\")\n",
    "    print(\"=\"*100)\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Saving the result to a CSV file\n",
    "    result.to_csv(f'{filename_prefix}.csv', index=False)\n",
    "    print(f\"\\nResults saved to {filename_prefix}.csv\")\n",
    "    \n",
    "    # Sorting the dataframe on accuracy and F1 Score\n",
    "    sorted_result = result.sort_values(by=['Accuracy', 'F1 Score'], ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\")\n",
    "    print(\"=\"*100)\n",
    "    print(sorted_result.to_string(index=False))\n",
    "    \n",
    "    # Saving the sorted result to a CSV file\n",
    "    sorted_result.to_csv(f'sorted_{filename_prefix}.csv', index=False)\n",
    "    print(f\"\\nSorted results saved to sorted_{filename_prefix}.csv\")\n",
    "    \n",
    "    return result, sorted_result\n",
    "\n",
    "# Function to clear results (useful when running multiple experiments)\n",
    "def clearResults():\n",
    "    \"\"\"Clear all stored results\"\"\"\n",
    "    global ML_Model, ML_Config, accuracy, f1_score, recall, precision, auc_roc\n",
    "    ML_Model.clear()\n",
    "    ML_Config.clear()\n",
    "    accuracy.clear()\n",
    "    f1_score.clear()\n",
    "    recall.clear()\n",
    "    precision.clear()\n",
    "    auc_roc.clear()\n",
    "    print(\"Results cleared!\")\n",
    "\n",
    "# Function to plot model comparison\n",
    "def plotModelComparison(result_df):\n",
    "    \"\"\"\n",
    "    Create visualization comparing model performances\n",
    "    \n",
    "    Parameters:\n",
    "    result_df: DataFrame with model results\n",
    "    \"\"\"\n",
    "    # Convert percentage strings back to floats for plotting\n",
    "    metrics_cols = ['Accuracy', 'F1 Score', 'Recall', 'Precision', 'ROC_AUC']\n",
    "    plot_df = result_df.copy()\n",
    "    \n",
    "    for col in metrics_cols:\n",
    "        plot_df[col] = plot_df[col].str.rstrip('%').astype(float)\n",
    "    \n",
    "    # Create subplot for each metric\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_cols):\n",
    "        # Group by model and get mean performance across configurations\n",
    "        model_performance = plot_df.groupby('ML Model')[metric].mean().sort_values(ascending=False)\n",
    "        \n",
    "        # Create bar plot\n",
    "        ax = axes[idx]\n",
    "        bars = ax.bar(range(len(model_performance)), model_performance.values, \n",
    "                      color=plt.cm.Blues(np.linspace(0.4, 0.9, len(model_performance))))\n",
    "        ax.set_xticks(range(len(model_performance)))\n",
    "        ax.set_xticklabels(model_performance.index, rotation=45, ha='right')\n",
    "        ax.set_ylabel(f'{metric} (%)')\n",
    "        ax.set_title(f'Average {metric} by Model', fontweight='bold')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # Hide the last subplot if we have 5 metrics\n",
    "    if len(metrics_cols) == 5:\n",
    "        axes[5].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Model results storage framework loaded successfully!\")\n",
    "print(\"Available functions:\")\n",
    "print(\"- storeResults(model, config, accuracy, f1, recall, precision, auc_roc)\")\n",
    "print(\"- displayAndSaveResults(filename_prefix='model_results')\")\n",
    "print(\"- clearResults()\")\n",
    "print(\"- plotModelComparison(result_df)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28c6989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stress type encoding:\n",
      "  Distress (Negative Stress) - Stress that causes anxiety and impairs well-being.: 0\n",
      "  Eustress (Positive Stress) - Stress that motivates and enhances performance.: 1\n",
      "  No Stress - Currently experiencing minimal to no stress.: 2\n",
      "\n",
      "X.dtypes after processing:\n",
      " Gender                                                                  int64\n",
      "Age                                                                     int64\n",
      "Have you recently experienced stress in your life?                      int64\n",
      "Have you noticed a rapid heartbeat or palpitations?                     int64\n",
      "Have you been dealing with anxiety or tension recently?                 int64\n",
      "Do you face any sleep problems or difficulties falling asleep?          int64\n",
      "Have you been dealing with anxiety or tension recently?.1               int64\n",
      "Have you been getting headaches more often than usual?                  int64\n",
      "Do you get irritated easily?                                            int64\n",
      "Do you have trouble concentrating on your academic tasks?               int64\n",
      "Have you been feeling sadness or low mood?                              int64\n",
      "Have you been experiencing any illness or health issues?                int64\n",
      "Do you often feel lonely or isolated?                                   int64\n",
      "Do you feel overwhelmed with your academic workload?                    int64\n",
      "Are you in competition with your peers, and does it affect you?         int64\n",
      "Do you find that your relationship often causes you stress?             int64\n",
      "Are you facing any difficulties with your professors or instructors?    int64\n",
      "Is your working environment unpleasant or stressful?                    int64\n",
      "Do you struggle to find time for relaxation and leisure activities?     int64\n",
      "Is your hostel or home environment causing you difficulties?            int64\n",
      "Do you lack confidence in your academic performance?                    int64\n",
      "Do you lack confidence in your choice of academic subjects?             int64\n",
      "Academic and extracurricular activities conflicting for you?            int64\n",
      "Do you attend classes regularly?                                        int64\n",
      "Have you gained/lost weight?                                            int64\n",
      "dtype: object\n",
      "\n",
      "Stress type class counts:\n",
      " 0     32\n",
      "1    768\n",
      "2     43\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi\n",
    "# 2025-07-22\n",
    "# Load, process and split stress dataset using stress type as target variable\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/Stress_Dataset.csv')\n",
    "\n",
    "# Define target variable (stress type)\n",
    "target_col = 'Which type of stress do you primarily experience?'\n",
    "y = df[target_col]\n",
    "\n",
    "# Separate features (all columns except target)\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Label encode the target variable (stress type)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Print encoding mapping\n",
    "print(\"Stress type encoding:\")\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    print(f\"  {class_name}: {i}\")\n",
    "\n",
    "# Print result info\n",
    "print(\"\\nX.dtypes after processing:\\n\", X.dtypes)\n",
    "print(\"\\nStress type class counts:\\n\", pd.Series(y).value_counts().sort_index())\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c849cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892602bb",
   "metadata": {},
   "source": [
    "### SVM with PCA 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703b85da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 24\n",
      "\n",
      "=== RFECV Feature Selection with SVM ===\n",
      "Optimal number of features selected by RFECV: 24\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 90.0% variance: 19\n",
      "\n",
      "=== SVM Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running SVM with Original Data configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.998418  0.992619 0.986111   0.999422 0.999948\n",
      "    Test  0.981043  0.921710 0.867424   0.993197 0.997537\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 0.01, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "Running SVM with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.992089  0.969245 0.944444   0.997131 0.999990\n",
      "    Test  0.981043  0.921710 0.867424   0.993197 0.996918\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 10, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "Running SVM with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.992089  0.969245 0.944444   0.997131 0.999990\n",
      "    Test  0.981043  0.921710 0.867424   0.993197 0.996644\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 10, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "Running SVM with RFECV configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.992089  0.969245 0.944444   0.997131 0.999990\n",
      "    Test  0.981043  0.921710 0.867424   0.993197 0.996462\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 10, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "Running SVM with PCA configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.992089  0.969245 0.944444   0.997131 0.999583\n",
      "    Test  0.990521  0.964940 0.939394   0.996564 0.999817\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 100, 'coef0': 0.5, 'degree': 2, 'gamma': 'auto', 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(SVC(kernel='linear'), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with SVM ===\")\n",
    "svm_estimator = SVC(kernel='linear')\n",
    "\n",
    "rfecv = RFECV(estimator=svm_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=svm_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance*100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: SVM + GridSearchCV\n",
    "print(\"\\n=== SVM Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'coef0': [0.0, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning SVM with {name} configuration...\")\n",
    "    svc = GridSearchCV(SVC(probability=True), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
    "    svc.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_svc = svc.predict(X_train_cfg)\n",
    "    y_test_svc = svc.predict(X_test_cfg)\n",
    "    y_train_svc_proba = svc.predict_proba(X_train_cfg)\n",
    "    y_test_svc_proba = svc.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_svc),\n",
    "            metrics.accuracy_score(y_test, y_test_svc),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_svc, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_svc, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_svc, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_svc, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_svc, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_svc, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_svc_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_svc_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nSupport Vector Machine Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_svc_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'Support Vector Machine 90',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_svc),\n",
    "        metrics.f1_score(y_test, y_test_svc, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_svc, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_svc, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(svc.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50abe0",
   "metadata": {},
   "source": [
    "### SVM with PCA 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61756ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 24\n",
      "\n",
      "=== RFECV Feature Selection with SVM ===\n",
      "Optimal number of features selected by RFECV: 24\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 95.0% variance: 22\n",
      "\n",
      "=== SVM Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running SVM with Original Data configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.998418  0.992619 0.986111   0.999422 0.999969\n",
      "    Test  0.981043  0.921710 0.867424   0.993197 0.997445\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 0.01, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "Running SVM with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.992089  0.969245 0.944444   0.997131 1.000000\n",
      "    Test  0.981043  0.921710 0.867424   0.993197 0.997101\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 10, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "Running SVM with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.992089  0.969245 0.944444   0.997131 0.999990\n",
      "    Test  0.981043  0.921710 0.867424   0.993197 0.996462\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 10, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "Running SVM with RFECV configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.992089  0.969245 0.944444   0.997131 0.999990\n",
      "    Test  0.981043  0.921710 0.867424   0.993197 0.996462\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 10, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "Running SVM with PCA configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.995253  0.981289 0.965278   0.998273 1.000000\n",
      "    Test  0.985782  0.941860 0.897727   0.994872 0.996439\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 100, 'coef0': 0.1, 'degree': 2, 'gamma': 'auto', 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(SVC(kernel='linear'), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with SVM ===\")\n",
    "svm_estimator = SVC(kernel='linear')\n",
    "\n",
    "rfecv = RFECV(estimator=svm_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=svm_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.95\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance*100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: SVM + GridSearchCV\n",
    "print(\"\\n=== SVM Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'coef0': [0.0, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning SVM with {name} configuration...\")\n",
    "    svc = GridSearchCV(SVC(probability=True), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
    "    svc.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_svc = svc.predict(X_train_cfg)\n",
    "    y_test_svc = svc.predict(X_test_cfg)\n",
    "    y_train_svc_proba = svc.predict_proba(X_train_cfg)\n",
    "    y_test_svc_proba = svc.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_svc),\n",
    "            metrics.accuracy_score(y_test, y_test_svc),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_svc, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_svc, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_svc, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_svc, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_svc, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_svc, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_svc_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_svc_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nSupport Vector Machine Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_svc_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'Support Vector Machine 95',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_svc),\n",
    "        metrics.f1_score(y_test, y_test_svc, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_svc, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_svc, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(svc.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822a9fe",
   "metadata": {},
   "source": [
    "### SVM with PCA 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ea0372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 24\n",
      "\n",
      "=== RFECV Feature Selection with SVM ===\n",
      "Optimal number of features selected by RFECV: 24\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 99.0% variance: 24\n",
      "\n",
      "=== SVM Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running SVM with Original Data configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.998418  0.992619 0.986111   0.999422 0.999969\n",
      "    Test  0.981043  0.921710 0.867424   0.993197 0.996531\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 0.01, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "Running SVM with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.992089  0.969245 0.944444   0.997131 0.999990\n",
      "    Test  0.981043  0.921710 0.867424   0.993197 0.996918\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 10, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "Running SVM with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.992089  0.969245 0.944444   0.997131 0.999969\n",
      "    Test  0.981043  0.921710 0.867424   0.993197 0.996462\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 10, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "Running SVM with RFECV configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.992089  0.969245 0.944444   0.997131 1.000000\n",
      "    Test  0.981043  0.921710 0.867424   0.993197 0.996462\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 10, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "Running SVM with PCA configuration...\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "\n",
      "Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.998418  0.992619 0.986111   0.999422 0.999637\n",
      "    Test  0.981043  0.930827 0.924558   0.940972 0.998723\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'C': 100, 'coef0': 0.0, 'degree': 2, 'gamma': 0.1, 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(SVC(kernel='linear'), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with SVM ===\")\n",
    "svm_estimator = SVC(kernel='linear')\n",
    "\n",
    "rfecv = RFECV(estimator=svm_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=svm_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.99\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance*100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: SVM + GridSearchCV\n",
    "print(\"\\n=== SVM Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'coef0': [0.0, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning SVM with {name} configuration...\")\n",
    "    svc = GridSearchCV(SVC(probability=True), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
    "    svc.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_svc = svc.predict(X_train_cfg)\n",
    "    y_test_svc = svc.predict(X_test_cfg)\n",
    "    y_train_svc_proba = svc.predict_proba(X_train_cfg)\n",
    "    y_test_svc_proba = svc.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_svc),\n",
    "            metrics.accuracy_score(y_test, y_test_svc),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_svc, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_svc, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_svc, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_svc, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_svc, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_svc, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_svc_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_svc_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nSupport Vector Machine Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_svc_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'Support Vector Machine 99',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_svc),\n",
    "        metrics.f1_score(y_test, y_test_svc, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_svc, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_svc, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(svc.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce270e1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f560a",
   "metadata": {},
   "source": [
    "### Random Forest with PCA 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff8fe52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 17\n",
      "\n",
      "=== RFECV Feature Selection with Random Forest ===\n",
      "Optimal number of features selected by RFECV: 17\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 90.0% variance: 14\n",
      "\n",
      "=== Random Forest Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Random Forest with Original Data configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.933649  0.599386 0.530303   0.977346 0.979634\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Running Random Forest with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000  1.00000\n",
      "    Test  0.933649  0.599386 0.530303   0.977346  0.97968\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Running Random Forest with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.947867  0.723318 0.632576   0.981938 0.962434\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Running Random Forest with RFECV configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.947867  0.723318 0.632576   0.981938 0.962434\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Running Random Forest with PCA configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.943128  0.688312 0.590909   0.980392 0.945081\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, Random Forest classification with preprocessing and result logging\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(RandomForestClassifier(random_state=42), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with Random Forest ===\")\n",
    "rf_estimator = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rfecv = RFECV(estimator=rf_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=rf_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: Random Forest + GridSearchCV\n",
    "print(\"\\n=== Random Forest Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200,300,400],\n",
    "    'max_depth': [10,20,50, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt'],\n",
    "    'bootstrap': [True],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Random Forest with {name} configuration...\")\n",
    "    rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
    "    rf.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_rf = rf.predict(X_train_cfg)\n",
    "    y_test_rf = rf.predict(X_test_cfg)\n",
    "    y_train_rf_proba = rf.predict_proba(X_train_cfg)\n",
    "    y_test_rf_proba = rf.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_rf),\n",
    "            metrics.accuracy_score(y_test, y_test_rf),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_rf, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_rf, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_rf, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_rf_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nRandom Forest Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'Random Forest 90',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_rf),\n",
    "        metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c96ed2",
   "metadata": {},
   "source": [
    "### Random Forest with PCA 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5784085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 17\n",
      "\n",
      "=== RFECV Feature Selection with Random Forest ===\n",
      "Optimal number of features selected by RFECV: 17\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 95.0% variance: 16\n",
      "\n",
      "=== Random Forest Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Random Forest with Original Data configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.933649  0.599386 0.530303   0.977346 0.979634\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Running Random Forest with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000  1.00000\n",
      "    Test  0.933649  0.599386 0.530303   0.977346  0.97968\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Running Random Forest with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.947867  0.723318 0.632576   0.981938 0.962434\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Running Random Forest with RFECV configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.947867  0.723318 0.632576   0.981938 0.962434\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Running Random Forest with PCA configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.988670 0.979167   0.998847 1.000000\n",
      "    Test  0.943128  0.682828 0.579545   0.980392 0.942089\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, Random Forest classification with preprocessing and result logging\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(RandomForestClassifier(random_state=42), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with Random Forest ===\")\n",
    "rf_estimator = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rfecv = RFECV(estimator=rf_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=rf_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.95\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: Random Forest + GridSearchCV\n",
    "print(\"\\n=== Random Forest Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200,300,400],\n",
    "    'max_depth': [10,20,50, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt'],\n",
    "    'bootstrap': [True],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Random Forest with {name} configuration...\")\n",
    "    rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
    "    rf.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_rf = rf.predict(X_train_cfg)\n",
    "    y_test_rf = rf.predict(X_test_cfg)\n",
    "    y_train_rf_proba = rf.predict_proba(X_train_cfg)\n",
    "    y_test_rf_proba = rf.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_rf),\n",
    "            metrics.accuracy_score(y_test, y_test_rf),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_rf, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_rf, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_rf, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_rf_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nRandom Forest Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'Random Forest 95',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_rf),\n",
    "        metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6a460",
   "metadata": {},
   "source": [
    "### Random Forest with PCA 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acfd1a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 17\n",
      "\n",
      "=== RFECV Feature Selection with Random Forest ===\n",
      "Optimal number of features selected by RFECV: 17\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 99.0% variance: 17\n",
      "\n",
      "=== Random Forest Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Random Forest with Original Data configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.933649  0.599386 0.530303   0.977346 0.979634\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Running Random Forest with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000  1.00000\n",
      "    Test  0.933649  0.599386 0.530303   0.977346  0.97968\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Running Random Forest with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.947867  0.723318 0.632576   0.981938 0.962434\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Running Random Forest with RFECV configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.947867  0.723318 0.632576   0.981938 0.962434\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Running Random Forest with PCA configuration...\n",
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "\n",
      "Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.943128  0.688312 0.590909   0.980392 0.963099\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, Random Forest classification with preprocessing and result logging\n",
    "\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(RandomForestClassifier(random_state=42), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with Random Forest ===\")\n",
    "rf_estimator = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rfecv = RFECV(estimator=rf_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=rf_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.99\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: Random Forest + GridSearchCV\n",
    "print(\"\\n=== Random Forest Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200,300,400],\n",
    "    'max_depth': [10,20,50, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt'],\n",
    "    'bootstrap': [True],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Random Forest with {name} configuration...\")\n",
    "    rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
    "    rf.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_rf = rf.predict(X_train_cfg)\n",
    "    y_test_rf = rf.predict(X_test_cfg)\n",
    "    y_train_rf_proba = rf.predict_proba(X_train_cfg)\n",
    "    y_test_rf_proba = rf.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_rf),\n",
    "            metrics.accuracy_score(y_test, y_test_rf),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_rf, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_rf, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_rf, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_rf_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nRandom Forest Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_rf_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'Random Forest 99',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_rf),\n",
    "        metrics.f1_score(y_test, y_test_rf, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_rf, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_rf, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16942ac3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0f7b8",
   "metadata": {},
   "source": [
    "### Gradient Boosting with PCA 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9fe6d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 14\n",
      "\n",
      "=== RFECV Feature Selection with Gradient Boosting ===\n",
      "Optimal number of features selected by RFECV: 14\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 90.0% variance: 12\n",
      "\n",
      "=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Gradient Boosting with Original Data configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.966825  0.848374 0.765152   0.988275 0.990586\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 500, 'subsample': 0.8}\n",
      "\n",
      "Running Gradient Boosting with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000  1.00000\n",
      "    Test  0.957346  0.790443 0.693182   0.985075  0.98958\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "Running Gradient Boosting with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987328 0.989005   0.986088 0.999959\n",
      "    Test  0.952607  0.769274 0.702809   0.937381 0.942574\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 500, 'subsample': 0.8}\n",
      "\n",
      "Running Gradient Boosting with RFECV configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987201 0.985532   0.989320 0.999959\n",
      "    Test  0.952607  0.769274 0.702809   0.937381 0.953654\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "\n",
      "Running Gradient Boosting with PCA configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987201 0.985532   0.989320 0.999959\n",
      "    Test  0.933649  0.599386 0.530303   0.977346 0.923748\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 500, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, Gradient Boosting classification with preprocessing and result logging\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(GradientBoostingClassifier(), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with Gradient Boosting ===\")\n",
    "gbc_estimator = GradientBoostingClassifier()\n",
    "\n",
    "rfecv = RFECV(estimator=gbc_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=gbc_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: Gradient Boosting + GridSearchCV\n",
    "print(\"\\n=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200,500],\n",
    "    'max_depth': [3, 5,7,9,15, 21],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'subsample': [0.8],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Gradient Boosting with {name} configuration...\")\n",
    "    gbc = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
    "    gbc.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_gbc = gbc.predict(X_train_cfg)\n",
    "    y_test_gbc = gbc.predict(X_test_cfg)\n",
    "    y_train_gbc_proba = gbc.predict_proba(X_train_cfg)\n",
    "    y_test_gbc_proba = gbc.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_gbc),\n",
    "            metrics.accuracy_score(y_test, y_test_gbc),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_gbc, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_gbc, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_gbc, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_gbc, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_gbc, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_gbc, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_gbc_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gbc_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nGradient Boosting Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gbc_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'Gradient Boosting 90',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_gbc),\n",
    "        metrics.f1_score(y_test, y_test_gbc, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_gbc, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_gbc, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(gbc.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9909e6",
   "metadata": {},
   "source": [
    "### Gradient Boosting with PCA 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1894f6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 16\n",
      "\n",
      "=== RFECV Feature Selection with Gradient Boosting ===\n",
      "Optimal number of features selected by RFECV: 16\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 95.0% variance: 15\n",
      "\n",
      "=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Gradient Boosting with Original Data configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000   1.00000 1.000000    1.00000 1.000000\n",
      "    Test  0.947867   0.74708 0.661143    0.91675 0.986373\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "Running Gradient Boosting with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.947867  0.723318 0.632576   0.981938 0.983715\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "\n",
      "Running Gradient Boosting with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987328 0.989005   0.986088 0.999959\n",
      "    Test  0.952607  0.759061 0.662879   0.983498 0.954009\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "Running Gradient Boosting with RFECV configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987201 0.985532   0.989320 0.999959\n",
      "    Test  0.962085  0.818235 0.723485   0.986667 0.967974\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "Running Gradient Boosting with PCA configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987201 0.985532   0.989320 0.999959\n",
      "    Test  0.938389  0.647204 0.560606   0.978862 0.911336\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 9, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, Gradient Boosting classification with preprocessing and result logging\n",
    "\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(GradientBoostingClassifier(), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with Gradient Boosting ===\")\n",
    "gbc_estimator = GradientBoostingClassifier()\n",
    "\n",
    "rfecv = RFECV(estimator=gbc_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=gbc_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.95\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: Gradient Boosting + GridSearchCV\n",
    "print(\"\\n=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200,500],\n",
    "    'max_depth': [3, 5,7,9,15, 21],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'subsample': [0.8],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Gradient Boosting with {name} configuration...\")\n",
    "    gbc = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
    "    gbc.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_gbc = gbc.predict(X_train_cfg)\n",
    "    y_test_gbc = gbc.predict(X_test_cfg)\n",
    "    y_train_gbc_proba = gbc.predict_proba(X_train_cfg)\n",
    "    y_test_gbc_proba = gbc.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_gbc),\n",
    "            metrics.accuracy_score(y_test, y_test_gbc),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_gbc, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_gbc, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_gbc, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_gbc, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_gbc, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_gbc, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_gbc_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gbc_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nGradient Boosting Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gbc_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'Gradient Boosting 95',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_gbc),\n",
    "        metrics.f1_score(y_test, y_test_gbc, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_gbc, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_gbc, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(gbc.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12584b8b",
   "metadata": {},
   "source": [
    "### Gradient Boosting with PCA 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2c47df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 14\n",
      "\n",
      "=== RFECV Feature Selection with Gradient Boosting ===\n",
      "Optimal number of features selected by RFECV: 14\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 99.0% variance: 14\n",
      "\n",
      "=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Gradient Boosting with Original Data configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.962085  0.820578 0.734848   0.986667 0.990144\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 500, 'subsample': 0.8}\n",
      "\n",
      "Running Gradient Boosting with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000  1.00000\n",
      "    Test  0.962085  0.820578 0.734848   0.986667  0.98729\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "Running Gradient Boosting with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987039 0.975694   0.998847 0.999959\n",
      "    Test  0.957346  0.800661 0.733112   0.938981 0.957123\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "Running Gradient Boosting with RFECV configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987039 0.975694   0.998847 0.999959\n",
      "    Test  0.947867  0.733527 0.672506   0.935797 0.956817\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "Running Gradient Boosting with PCA configuration...\n",
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n",
      "\n",
      "Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987039 0.975694   0.998847 0.999959\n",
      "    Test  0.938389  0.634384 0.571970   0.978862 0.951643\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, Gradient Boosting classification with preprocessing and result logging\n",
    "\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(GradientBoostingClassifier(), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with Gradient Boosting ===\")\n",
    "gbc_estimator = GradientBoostingClassifier()\n",
    "\n",
    "rfecv = RFECV(estimator=gbc_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=gbc_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.99\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: Gradient Boosting + GridSearchCV\n",
    "print(\"\\n=== Gradient Boosting Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200,500],\n",
    "    'max_depth': [3, 5,7,9,15, 21],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'subsample': [0.8],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Gradient Boosting with {name} configuration...\")\n",
    "    gbc = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=10, n_jobs=-1, verbose=2)\n",
    "    gbc.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_gbc = gbc.predict(X_train_cfg)\n",
    "    y_test_gbc = gbc.predict(X_test_cfg)\n",
    "    y_train_gbc_proba = gbc.predict_proba(X_train_cfg)\n",
    "    y_test_gbc_proba = gbc.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_gbc),\n",
    "            metrics.accuracy_score(y_test, y_test_gbc),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_gbc, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_gbc, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_gbc, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_gbc, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_gbc, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_gbc, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_gbc_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gbc_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nGradient Boosting Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_gbc_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'Gradient Boosting 99',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_gbc),\n",
    "        metrics.f1_score(y_test, y_test_gbc, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_gbc, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_gbc, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(gbc.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06f7fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8f838",
   "metadata": {},
   "source": [
    "### Adaboost with PCA 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb4da461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 25\n",
      "\n",
      "=== RFECV Feature Selection with AdaBoost ===\n",
      "Optimal number of features selected by RFECV: 11\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 90.0% variance: 10\n",
      "\n",
      "=== AdaBoost Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running AdaBoost with Original Data configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.966825  0.860668 0.805082   0.942229 0.992096\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 200}\n",
      "\n",
      "Running AdaBoost with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.966825  0.860668 0.805082   0.942229 0.992096\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 200}\n",
      "\n",
      "Running AdaBoost with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000  1.00000\n",
      "    Test  0.962085  0.818235 0.723485   0.986667  0.98906\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 200}\n",
      "\n",
      "Running AdaBoost with RFECV configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.984177  0.943348 0.935185   0.952095 0.998158\n",
      "    Test  0.947867  0.747080 0.661143   0.916750 0.967564\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 0.1, 'n_estimators': 200}\n",
      "\n",
      "Running AdaBoost with PCA configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.938389  0.693736 0.629104   0.842869 0.961022\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=5), 'learning_rate': 1, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, AdaBoost classification with preprocessing and result logging\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(AdaBoostClassifier(), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with AdaBoost ===\")\n",
    "ab_estimator = AdaBoostClassifier()\n",
    "\n",
    "rfecv = RFECV(estimator=ab_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=ab_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: AdaBoost + GridSearchCV\n",
    "print(\"\\n=== AdaBoost Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 1],\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "    'estimator': [DecisionTreeClassifier(max_depth=d) for d in [1, 3, 5]]\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning AdaBoost with {name} configuration...\")\n",
    "    ab = GridSearchCV(\n",
    "        AdaBoostClassifier(),\n",
    "        param_grid,\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    ab.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_ab = ab.predict(X_train_cfg)\n",
    "    y_test_ab = ab.predict(X_test_cfg)\n",
    "    y_train_ab_proba = ab.predict_proba(X_train_cfg)\n",
    "    y_test_ab_proba = ab.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_ab),\n",
    "            metrics.accuracy_score(y_test, y_test_ab),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_ab, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_ab, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_ab, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_ab, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_ab, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_ab, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_ab_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_ab_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nAdaBoost Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_ab_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'AdaBoost 90',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_ab),\n",
    "        metrics.f1_score(y_test, y_test_ab, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_ab, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_ab, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(ab.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac54dd",
   "metadata": {},
   "source": [
    "### Adaboost with PCA 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2931c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 25\n",
      "\n",
      "=== RFECV Feature Selection with AdaBoost ===\n",
      "Optimal number of features selected by RFECV: 11\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 95.0% variance: 10\n",
      "\n",
      "=== AdaBoost Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running AdaBoost with Original Data configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.957346  0.790443 0.693182   0.985075 0.984545\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 100}\n",
      "\n",
      "Running AdaBoost with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.957346  0.790443 0.693182   0.985075 0.984545\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 100}\n",
      "\n",
      "Running AdaBoost with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000  1.00000\n",
      "    Test  0.962085  0.818235 0.723485   0.986667  0.98906\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 200}\n",
      "\n",
      "Running AdaBoost with RFECV configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training   1.00000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test   0.92891  0.698926 0.654198   0.760857 0.942082\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 50}\n",
      "\n",
      "Running AdaBoost with PCA configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.966825  0.853261 0.793718   0.942229 0.970545\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, AdaBoost classification with preprocessing and result logging\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(AdaBoostClassifier(), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with AdaBoost ===\")\n",
    "ab_estimator = AdaBoostClassifier()\n",
    "\n",
    "rfecv = RFECV(estimator=ab_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=ab_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.95\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: AdaBoost + GridSearchCV\n",
    "print(\"\\n=== AdaBoost Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 1],\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "    'estimator': [DecisionTreeClassifier(max_depth=d) for d in [1, 3, 5]]\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning AdaBoost with {name} configuration...\")\n",
    "    ab = GridSearchCV(\n",
    "        AdaBoostClassifier(),\n",
    "        param_grid,\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    ab.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_ab = ab.predict(X_train_cfg)\n",
    "    y_test_ab = ab.predict(X_test_cfg)\n",
    "    y_train_ab_proba = ab.predict_proba(X_train_cfg)\n",
    "    y_test_ab_proba = ab.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_ab),\n",
    "            metrics.accuracy_score(y_test, y_test_ab),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_ab, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_ab, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_ab, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_ab, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_ab, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_ab, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_ab_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_ab_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nAdaBoost Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_ab_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'AdaBoost 95',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_ab),\n",
    "        metrics.f1_score(y_test, y_test_ab, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_ab, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_ab, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(ab.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad91dc7",
   "metadata": {},
   "source": [
    "### Adaboost with PCA 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c96e1c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 25\n",
      "\n",
      "=== RFECV Feature Selection with AdaBoost ===\n",
      "Optimal number of features selected by RFECV: 11\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 99.0% variance: 11\n",
      "\n",
      "=== AdaBoost Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running AdaBoost with Original Data configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.957346  0.804293 0.721749   0.938981 0.990977\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 200}\n",
      "\n",
      "Running AdaBoost with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000  1.00000\n",
      "    Test  0.962085  0.818235 0.723485   0.986667  0.98906\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 200}\n",
      "\n",
      "Running AdaBoost with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.966825  0.858663 0.793718   0.948181 0.989472\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 100}\n",
      "\n",
      "Running AdaBoost with RFECV configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
      "Training  1.000000   1.00000 1.00000   1.000000 1.000000\n",
      "    Test  0.938389   0.71114 0.65767   0.821345 0.941128\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=5), 'learning_rate': 1, 'n_estimators': 100}\n",
      "\n",
      "Running AdaBoost with PCA configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.00000   1.000000 1.000000\n",
      "    Test  0.947867  0.775759 0.72964   0.843606 0.962821\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, AdaBoost classification with preprocessing and result logging\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(AdaBoostClassifier(), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with AdaBoost ===\")\n",
    "ab_estimator = AdaBoostClassifier()\n",
    "\n",
    "rfecv = RFECV(estimator=ab_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=ab_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.99\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: AdaBoost + GridSearchCV\n",
    "print(\"\\n=== AdaBoost Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 1],\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "    'estimator': [DecisionTreeClassifier(max_depth=d) for d in [1, 3, 5]]\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning AdaBoost with {name} configuration...\")\n",
    "    ab = GridSearchCV(\n",
    "        AdaBoostClassifier(),\n",
    "        param_grid,\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    ab.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_ab = ab.predict(X_train_cfg)\n",
    "    y_test_ab = ab.predict(X_test_cfg)\n",
    "    y_train_ab_proba = ab.predict_proba(X_train_cfg)\n",
    "    y_test_ab_proba = ab.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_ab),\n",
    "            metrics.accuracy_score(y_test, y_test_ab),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_ab, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_ab, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_ab, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_ab, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_ab, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_ab, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_ab_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_ab_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nAdaBoost Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_ab_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'AdaBoost 99',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_ab),\n",
    "        metrics.f1_score(y_test, y_test_ab, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_ab, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_ab, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(ab.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911a9a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76533d1",
   "metadata": {},
   "source": [
    "### XGBoost with PCA 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90fa3173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 15\n",
      "\n",
      "=== RFECV Feature Selection with XGBoost ===\n",
      "Optimal number of features selected by RFECV: 15\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 90.0% variance: 13\n",
      "\n",
      "=== XGBoost Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running XGBoost with Original Data configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.952607  0.755429 0.651515   0.983498 0.986044\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.952607  0.755429 0.651515   0.983498 0.986044\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987039 0.975694   0.998847 0.999959\n",
      "    Test  0.952607  0.753445 0.674242   0.983498 0.956643\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with RFECV configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987039 0.975694   0.998847 0.999959\n",
      "    Test  0.952607  0.753445 0.674242   0.983498 0.956643\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with PCA configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973101  0.883080 0.818866   0.975103 0.991660\n",
      "    Test  0.943128  0.682207 0.602273   0.980392 0.949287\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, XGBoost classification with preprocessing and result logging\n",
    "\n",
    "\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), \n",
    "                            X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with XGBoost ===\")\n",
    "xgb_estimator = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "rfecv = RFECV(estimator=xgb_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=xgb_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: XGBoost + GridSearchCV\n",
    "print(\"\\n=== XGBoost Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning XGBoost with {name} configuration...\")\n",
    "    xgb = GridSearchCV(\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        param_grid,\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    xgb.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_xgb = xgb.predict(X_train_cfg)\n",
    "    y_test_xgb = xgb.predict(X_test_cfg)\n",
    "    y_train_xgb_proba = xgb.predict_proba(X_train_cfg)\n",
    "    y_test_xgb_proba = xgb.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_xgb),\n",
    "            metrics.accuracy_score(y_test, y_test_xgb),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_xgb, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_xgb, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_xgb, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_xgb, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_xgb, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_xgb, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_xgb_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xgb_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nXGBoost Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xgb_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'XGBoost 90',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_xgb),\n",
    "        metrics.f1_score(y_test, y_test_xgb, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_xgb, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_xgb, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(xgb.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17717807",
   "metadata": {},
   "source": [
    "### XGBoost with PCA 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4be3de3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 15\n",
      "\n",
      "=== RFECV Feature Selection with XGBoost ===\n",
      "Optimal number of features selected by RFECV: 15\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 95.0% variance: 14\n",
      "\n",
      "=== XGBoost Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running XGBoost with Original Data configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.952607  0.755429 0.651515   0.983498 0.986044\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.952607  0.755429 0.651515   0.983498 0.986044\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987039 0.975694   0.998847 0.999959\n",
      "    Test  0.952607  0.753445 0.674242   0.983498 0.956643\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with RFECV configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987039 0.975694   0.998847 0.999959\n",
      "    Test  0.952607  0.753445 0.674242   0.983498 0.956643\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with PCA configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.969937  0.869930 0.807870   0.958666 0.987737\n",
      "    Test  0.938389  0.674871 0.640467   0.898425 0.921033\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, XGBoost classification with preprocessing and result logging\n",
    "\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), \n",
    "                            X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with XGBoost ===\")\n",
    "xgb_estimator = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "rfecv = RFECV(estimator=xgb_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=xgb_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.95\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: XGBoost + GridSearchCV\n",
    "print(\"\\n=== XGBoost Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning XGBoost with {name} configuration...\")\n",
    "    xgb = GridSearchCV(\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        param_grid,\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    xgb.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_xgb = xgb.predict(X_train_cfg)\n",
    "    y_test_xgb = xgb.predict(X_test_cfg)\n",
    "    y_train_xgb_proba = xgb.predict_proba(X_train_cfg)\n",
    "    y_test_xgb_proba = xgb.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_xgb),\n",
    "            metrics.accuracy_score(y_test, y_test_xgb),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_xgb, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_xgb, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_xgb, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_xgb, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_xgb, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_xgb, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_xgb_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xgb_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nXGBoost Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xgb_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'XGBoost 95',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_xgb),\n",
    "        metrics.f1_score(y_test, y_test_xgb, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_xgb, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_xgb, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(xgb.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4191db",
   "metadata": {},
   "source": [
    "### XGBoost with PCA 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "612e7366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 15\n",
      "\n",
      "=== RFECV Feature Selection with XGBoost ===\n",
      "Optimal number of features selected by RFECV: 15\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 99.0% variance: 15\n",
      "\n",
      "=== XGBoost Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running XGBoost with Original Data configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.952607  0.755429 0.651515   0.983498 0.986044\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.952607  0.755429 0.651515   0.983498 0.986044\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987039 0.975694   0.998847 0.999959\n",
      "    Test  0.952607  0.753445 0.674242   0.983498 0.956643\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with RFECV configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987039 0.975694   0.998847 0.999959\n",
      "    Test  0.952607  0.753445 0.674242   0.983498 0.956643\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with PCA configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.981013  0.922644 0.887731   0.966466 0.997345\n",
      "    Test  0.947867  0.723318 0.632576   0.981938 0.961429\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'min_child_weight': 5, 'n_estimators': 100, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, XGBoost classification with preprocessing and result logging\n",
    "\n",
    "\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), \n",
    "                            X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with XGBoost ===\")\n",
    "xgb_estimator = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "rfecv = RFECV(estimator=xgb_estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=xgb_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.99\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: XGBoost + GridSearchCV\n",
    "print(\"\\n=== XGBoost Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning XGBoost with {name} configuration...\")\n",
    "    xgb = GridSearchCV(\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        param_grid,\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    xgb.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_xgb = xgb.predict(X_train_cfg)\n",
    "    y_test_xgb = xgb.predict(X_test_cfg)\n",
    "    y_train_xgb_proba = xgb.predict_proba(X_train_cfg)\n",
    "    y_test_xgb_proba = xgb.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_xgb),\n",
    "            metrics.accuracy_score(y_test, y_test_xgb),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_xgb, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_xgb, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_xgb, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_xgb, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_xgb, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_xgb, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_xgb_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xgb_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nXGBoost Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_xgb_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'XGBoost 99',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_xgb),\n",
    "        metrics.f1_score(y_test, y_test_xgb, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_xgb, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_xgb, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(xgb.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78501f2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0264b",
   "metadata": {},
   "source": [
    "### Bagging classification with PCA 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "972b06ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 22\n",
      "\n",
      "=== RFECV Feature Selection with Bagging ===\n",
      "Optimal number of features selected by RFECV: 1\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 90.0% variance: 1\n",
      "\n",
      "=== Bagging Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Bagging with Original Data configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.981013  0.916868 0.857639   0.993197 0.999923\n",
      "    Test  0.933649  0.599386 0.530303   0.977346 0.954901\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(min_samples_split=10), 'max_features': 0.8, 'max_samples': 1.0, 'n_estimators': 100}\n",
      "\n",
      "Running Bagging with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.981013  0.916868 0.857639   0.993197 0.999849\n",
      "    Test  0.933649  0.599386 0.530303   0.977346 0.949225\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(min_samples_split=10), 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 100}\n",
      "\n",
      "Running Bagging with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.979430  0.907726 0.843750   0.992643 0.999750\n",
      "    Test  0.943128  0.688312 0.590909   0.980392 0.937465\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(min_samples_split=10), 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200}\n",
      "\n",
      "Running Bagging with RFECV configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.911392  0.317881 0.333333   0.303797 0.705290\n",
      "    Test  0.909953  0.317618 0.333333   0.303318 0.709804\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(max_depth=3), 'max_features': 0.6, 'max_samples': 0.6, 'n_estimators': 100}\n",
      "\n",
      "Running Bagging with PCA configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.911392  0.317881 0.333333   0.303797 0.706551\n",
      "    Test  0.909953  0.317618 0.333333   0.303318 0.713093\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(max_depth=3), 'max_features': 0.6, 'max_samples': 0.6, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, Bagging classification with preprocessing and result logging\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(BaggingClassifier(estimator=DecisionTreeClassifier()), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with Bagging ===\")\n",
    "# Use single DecisionTreeClassifier for RFECV to enable feature_importances_\n",
    "tree_estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=tree_estimator,\n",
    "    step=1,\n",
    "    cv=StratifiedKFold(5),\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=tree_estimator, n_features_to_select=rfecv.n_features_)\n",
    "\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: BaggingClassifier + GridSearchCV\n",
    "print(\"\\n=== Bagging Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_samples': [0.6, 0.8, 1.0],\n",
    "    'max_features': [0.6, 0.8, 1.0],\n",
    "    'bootstrap': [True],\n",
    "    'bootstrap_features': [False],\n",
    "    'estimator': [ \n",
    "        DecisionTreeClassifier(max_depth=3, min_samples_split=2),\n",
    "        DecisionTreeClassifier(max_depth=5, min_samples_split=5),\n",
    "        DecisionTreeClassifier(max_depth=None, min_samples_split=10)\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Bagging with {name} configuration...\")\n",
    "    bag = GridSearchCV(\n",
    "        BaggingClassifier(),\n",
    "        param_grid,\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    bag.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_bag = bag.predict(X_train_cfg)\n",
    "    y_test_bag = bag.predict(X_test_cfg)\n",
    "    y_train_bag_proba = bag.predict_proba(X_train_cfg)\n",
    "    y_test_bag_proba = bag.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_bag),\n",
    "            metrics.accuracy_score(y_test, y_test_bag),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_bag, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_bag, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_bag, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_bag, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_bag, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_bag, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_bag_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_bag_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nBagging Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_bag_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'Bagging 90',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_bag),\n",
    "        metrics.f1_score(y_test, y_test_bag, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_bag, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_bag, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(bag.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b7511",
   "metadata": {},
   "source": [
    "### Bagging classification with PCA 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f14f035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 10\n",
      "\n",
      "=== RFECV Feature Selection with Bagging ===\n",
      "Optimal number of features selected by RFECV: 2\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 95.0% variance: 2\n",
      "\n",
      "=== Bagging Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Bagging with Original Data configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.958861  0.794992 0.704861   0.985604 0.998462\n",
      "    Test  0.933649  0.599386 0.530303   0.977346 0.967648\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(max_depth=5, min_samples_split=5), 'max_features': 0.6, 'max_samples': 1.0, 'n_estimators': 150}\n",
      "\n",
      "Running Bagging with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.962025  0.815719 0.725694   0.986667 0.998742\n",
      "    Test  0.933649  0.599386 0.530303   0.977346 0.949401\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(max_depth=5, min_samples_split=5), 'max_features': 1.0, 'max_samples': 0.6, 'n_estimators': 100}\n",
      "\n",
      "Running Bagging with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.958861  0.800964 0.711227   0.965278 0.996363\n",
      "    Test  0.943128  0.688312 0.590909   0.980392 0.809759\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(min_samples_split=10), 'max_features': 1.0, 'max_samples': 0.8, 'n_estimators': 200}\n",
      "\n",
      "Running Bagging with RFECV configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.920886  0.516274 0.509838   0.526580 0.848390\n",
      "    Test  0.872038  0.347470 0.359375   0.336816 0.702292\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(max_depth=5, min_samples_split=5), 'max_features': 1.0, 'max_samples': 0.6, 'n_estimators': 100}\n",
      "\n",
      "Running Bagging with PCA configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.920886  0.516274 0.509838   0.526580 0.843728\n",
      "    Test  0.872038  0.347470 0.359375   0.336816 0.707771\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(max_depth=5, min_samples_split=5), 'max_features': 1.0, 'max_samples': 0.6, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, Bagging classification with preprocessing and result logging\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(BaggingClassifier(estimator=DecisionTreeClassifier()), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with Bagging ===\")\n",
    "# Use single DecisionTreeClassifier for RFECV to enable feature_importances_\n",
    "tree_estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=tree_estimator,\n",
    "    step=1,\n",
    "    cv=StratifiedKFold(5),\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=tree_estimator, n_features_to_select=rfecv.n_features_)\n",
    "\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.95\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: BaggingClassifier + GridSearchCV\n",
    "print(\"\\n=== Bagging Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_samples': [0.6, 0.8, 1.0],\n",
    "    'max_features': [0.6, 0.8, 1.0],\n",
    "    'bootstrap': [True],\n",
    "    'bootstrap_features': [False],\n",
    "    'estimator': [ \n",
    "        DecisionTreeClassifier(max_depth=3, min_samples_split=2),\n",
    "        DecisionTreeClassifier(max_depth=5, min_samples_split=5),\n",
    "        DecisionTreeClassifier(max_depth=None, min_samples_split=10)\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Bagging with {name} configuration...\")\n",
    "    bag = GridSearchCV(\n",
    "        BaggingClassifier(),\n",
    "        param_grid,\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    bag.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_bag = bag.predict(X_train_cfg)\n",
    "    y_test_bag = bag.predict(X_test_cfg)\n",
    "    y_train_bag_proba = bag.predict_proba(X_train_cfg)\n",
    "    y_test_bag_proba = bag.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_bag),\n",
    "            metrics.accuracy_score(y_test, y_test_bag),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_bag, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_bag, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_bag, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_bag, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_bag, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_bag, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_bag_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_bag_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nBagging Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_bag_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'Bagging 95',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_bag),\n",
    "        metrics.f1_score(y_test, y_test_bag, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_bag, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_bag, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(bag.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d444d",
   "metadata": {},
   "source": [
    "### Bagging classification with PCA 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "066459e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 13\n",
      "\n",
      "=== RFECV Feature Selection with Bagging ===\n",
      "Optimal number of features selected by RFECV: 1\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 99.0% variance: 1\n",
      "\n",
      "=== Bagging Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Bagging with Original Data configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.957278  0.783941 0.694444   0.985075 0.998444\n",
      "    Test  0.928910  0.558176 0.488636   0.975845 0.927392\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(max_depth=5, min_samples_split=5), 'max_features': 1.0, 'max_samples': 0.6, 'n_estimators': 100}\n",
      "\n",
      "Running Bagging with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.952532  0.751826 0.656250   0.983498 0.999227\n",
      "    Test  0.933649  0.599386 0.530303   0.977346 0.960423\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(min_samples_split=10), 'max_features': 1.0, 'max_samples': 0.6, 'n_estimators': 150}\n",
      "\n",
      "Running Bagging with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.974684  0.888283 0.822338   0.977637 0.998922\n",
      "    Test  0.943128  0.688312 0.590909   0.980392 0.886902\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(min_samples_split=10), 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 150}\n",
      "\n",
      "Running Bagging with RFECV configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.911392  0.317881 0.333333   0.303797 0.733710\n",
      "    Test  0.909953  0.317618 0.333333   0.303318 0.712548\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(max_depth=3), 'max_features': 0.6, 'max_samples': 0.6, 'n_estimators': 100}\n",
      "\n",
      "Running Bagging with PCA configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.911392  0.317881 0.333333   0.303797 0.733710\n",
      "    Test  0.909953  0.317618 0.333333   0.303318 0.712548\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(max_depth=3), 'max_features': 0.6, 'max_samples': 0.6, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2025-07-07, Bagging classification with preprocessing and result logging\n",
    "\n",
    "# Store different configurations\n",
    "configurations = []\n",
    "configurations.append(('Original Data', X_train, X_test, y_train))\n",
    "\n",
    "# Step 2: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "configurations.append(('Normalized Data', X_train_normalized, X_test_normalized, y_train))\n",
    "\n",
    "# Step 3.1: SelectKBest\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(BaggingClassifier(estimator=DecisionTreeClassifier()), X_train_kbest, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = scores.index(max(scores)) + 1\n",
    "print(f\"Optimal number of features to select using SelectKBest: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = kbest.fit_transform(X_train_normalized, y_train)\n",
    "X_test_kbest = kbest.transform(X_test_normalized)\n",
    "selected_features_kbest = X.columns[kbest.get_support()]\n",
    "configurations.append(('SelectKBest', X_train_kbest, X_test_kbest, y_train))\n",
    "\n",
    "# Step 3.2: RFECV\n",
    "print(\"\\n=== RFECV Feature Selection with Bagging ===\")\n",
    "# Use single DecisionTreeClassifier for RFECV to enable feature_importances_\n",
    "tree_estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=tree_estimator,\n",
    "    step=1,\n",
    "    cv=StratifiedKFold(5),\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features selected by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=tree_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = rfe.fit_transform(X_train_kbest, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_kbest)\n",
    "selected_features_rfe = selected_features_kbest[rfe.get_support()]\n",
    "configurations.append(('RFECV', X_train_rfe, X_test_rfe, y_train))\n",
    "\n",
    "# Step 3.3: PCA\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "desired_variance = 0.99\n",
    "n_components = np.argmax(cumulative_variance >= desired_variance) + 1\n",
    "print(f'Number of components that explain {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_rfe)\n",
    "X_test_pca = pca.transform(X_test_rfe)\n",
    "configurations.append(('PCA', X_train_pca, X_test_pca, y_train))\n",
    "\n",
    "# Step 4: BaggingClassifier + GridSearchCV\n",
    "print(\"\\n=== Bagging Model Performance with Hyperparameter Tuning ===\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_samples': [0.6, 0.8, 1.0],\n",
    "    'max_features': [0.6, 0.8, 1.0],\n",
    "    'bootstrap': [True],\n",
    "    'bootstrap_features': [False],\n",
    "    'estimator': [ \n",
    "        DecisionTreeClassifier(max_depth=3, min_samples_split=2),\n",
    "        DecisionTreeClassifier(max_depth=5, min_samples_split=5),\n",
    "        DecisionTreeClassifier(max_depth=None, min_samples_split=10)\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
    "    print(f\"\\nRunning Bagging with {name} configuration...\")\n",
    "    bag = GridSearchCV(\n",
    "        BaggingClassifier(),\n",
    "        param_grid,\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    bag.fit(X_train_cfg, y_train_cfg)\n",
    "\n",
    "    y_train_bag = bag.predict(X_train_cfg)\n",
    "    y_test_bag = bag.predict(X_test_cfg)\n",
    "    y_train_bag_proba = bag.predict_proba(X_train_cfg)\n",
    "    y_test_bag_proba = bag.predict_proba(X_test_cfg)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Accuracy\": [\n",
    "            metrics.accuracy_score(y_train_cfg, y_train_bag),\n",
    "            metrics.accuracy_score(y_test, y_test_bag),\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            metrics.f1_score(y_train_cfg, y_train_bag, average='macro'),\n",
    "            metrics.f1_score(y_test, y_test_bag, average='macro'),\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            metrics.recall_score(y_train_cfg, y_train_bag, average='macro'),\n",
    "            metrics.recall_score(y_test, y_test_bag, average='macro'),\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            metrics.precision_score(y_train_cfg, y_train_bag, average='macro'),\n",
    "            metrics.precision_score(y_test, y_test_bag, average='macro'),\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_bag_proba, multi_class='ovr', average='macro'),\n",
    "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_bag_proba, multi_class='ovr', average='macro'),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_dict)\n",
    "    print(\"\\nBagging Model Performance Metrics\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_bag_proba, multi_class='ovr', average='macro')\n",
    "    storeResults(\n",
    "        'Bagging 99',\n",
    "        name,\n",
    "        metrics.accuracy_score(y_test, y_test_bag),\n",
    "        metrics.f1_score(y_test, y_test_bag, average='macro'),\n",
    "        metrics.recall_score(y_test, y_test_bag, average='macro'),\n",
    "        metrics.precision_score(y_test, y_test_bag, average='macro'),\n",
    "        auc_score\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "    print(bag.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd3688",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cc80be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODEL PERFORMANCE RESULTS\n",
      "====================================================================================================\n",
      "                 ML Model   Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
      "Support Vector Machine 90   Original Data  98.104%  92.171% 86.742%   99.320% 99.754%\n",
      "Support Vector Machine 90 Normalized Data  98.104%  92.171% 86.742%   99.320% 99.692%\n",
      "Support Vector Machine 90     SelectKBest  98.104%  92.171% 86.742%   99.320% 99.664%\n",
      "Support Vector Machine 90           RFECV  98.104%  92.171% 86.742%   99.320% 99.646%\n",
      "Support Vector Machine 90             PCA  99.052%  96.494% 93.939%   99.656% 99.982%\n",
      "Support Vector Machine 95   Original Data  98.104%  92.171% 86.742%   99.320% 99.745%\n",
      "Support Vector Machine 95 Normalized Data  98.104%  92.171% 86.742%   99.320% 99.710%\n",
      "Support Vector Machine 95     SelectKBest  98.104%  92.171% 86.742%   99.320% 99.646%\n",
      "Support Vector Machine 95           RFECV  98.104%  92.171% 86.742%   99.320% 99.646%\n",
      "Support Vector Machine 95             PCA  98.578%  94.186% 89.773%   99.487% 99.644%\n",
      "Support Vector Machine 99   Original Data  98.104%  92.171% 86.742%   99.320% 99.653%\n",
      "Support Vector Machine 99 Normalized Data  98.104%  92.171% 86.742%   99.320% 99.692%\n",
      "Support Vector Machine 99     SelectKBest  98.104%  92.171% 86.742%   99.320% 99.646%\n",
      "Support Vector Machine 99           RFECV  98.104%  92.171% 86.742%   99.320% 99.646%\n",
      "Support Vector Machine 99             PCA  98.104%  93.083% 92.456%   94.097% 99.872%\n",
      "         Random Forest 90   Original Data  93.365%  59.939% 53.030%   97.735% 97.963%\n",
      "         Random Forest 90 Normalized Data  93.365%  59.939% 53.030%   97.735% 97.968%\n",
      "         Random Forest 90     SelectKBest  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 90           RFECV  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 90             PCA  94.313%  68.831% 59.091%   98.039% 94.508%\n",
      "         Random Forest 95   Original Data  93.365%  59.939% 53.030%   97.735% 97.963%\n",
      "         Random Forest 95 Normalized Data  93.365%  59.939% 53.030%   97.735% 97.968%\n",
      "         Random Forest 95     SelectKBest  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 95           RFECV  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 95             PCA  94.313%  68.283% 57.954%   98.039% 94.209%\n",
      "         Random Forest 99   Original Data  93.365%  59.939% 53.030%   97.735% 97.963%\n",
      "         Random Forest 99 Normalized Data  93.365%  59.939% 53.030%   97.735% 97.968%\n",
      "         Random Forest 99     SelectKBest  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 99           RFECV  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 99             PCA  94.313%  68.831% 59.091%   98.039% 96.310%\n",
      "     Gradient Boosting 90   Original Data  96.683%  84.837% 76.515%   98.828% 99.059%\n",
      "     Gradient Boosting 90 Normalized Data  95.735%  79.044% 69.318%   98.508% 98.958%\n",
      "     Gradient Boosting 90     SelectKBest  95.261%  76.927% 70.281%   93.738% 94.257%\n",
      "     Gradient Boosting 90           RFECV  95.261%  76.927% 70.281%   93.738% 95.365%\n",
      "     Gradient Boosting 90             PCA  93.365%  59.939% 53.030%   97.735% 92.375%\n",
      "     Gradient Boosting 95   Original Data  94.787%  74.708% 66.114%   91.675% 98.637%\n",
      "     Gradient Boosting 95 Normalized Data  94.787%  72.332% 63.258%   98.194% 98.371%\n",
      "     Gradient Boosting 95     SelectKBest  95.261%  75.906% 66.288%   98.350% 95.401%\n",
      "     Gradient Boosting 95           RFECV  96.209%  81.824% 72.349%   98.667% 96.797%\n",
      "     Gradient Boosting 95             PCA  93.839%  64.720% 56.061%   97.886% 91.134%\n",
      "     Gradient Boosting 99   Original Data  96.209%  82.058% 73.485%   98.667% 99.014%\n",
      "     Gradient Boosting 99 Normalized Data  96.209%  82.058% 73.485%   98.667% 98.729%\n",
      "     Gradient Boosting 99     SelectKBest  95.735%  80.066% 73.311%   93.898% 95.712%\n",
      "     Gradient Boosting 99           RFECV  94.787%  73.353% 67.251%   93.580% 95.682%\n",
      "     Gradient Boosting 99             PCA  93.839%  63.438% 57.197%   97.886% 95.164%\n",
      "              AdaBoost 90   Original Data  96.683%  86.067% 80.508%   94.223% 99.210%\n",
      "              AdaBoost 90 Normalized Data  96.683%  86.067% 80.508%   94.223% 99.210%\n",
      "              AdaBoost 90     SelectKBest  96.209%  81.824% 72.349%   98.667% 98.906%\n",
      "              AdaBoost 90           RFECV  94.787%  74.708% 66.114%   91.675% 96.756%\n",
      "              AdaBoost 90             PCA  93.839%  69.374% 62.910%   84.287% 96.102%\n",
      "              AdaBoost 95   Original Data  95.735%  79.044% 69.318%   98.508% 98.454%\n",
      "              AdaBoost 95 Normalized Data  95.735%  79.044% 69.318%   98.508% 98.454%\n",
      "              AdaBoost 95     SelectKBest  96.209%  81.824% 72.349%   98.667% 98.906%\n",
      "              AdaBoost 95           RFECV  92.891%  69.893% 65.420%   76.086% 94.208%\n",
      "              AdaBoost 95             PCA  96.683%  85.326% 79.372%   94.223% 97.055%\n",
      "              AdaBoost 99   Original Data  95.735%  80.429% 72.175%   93.898% 99.098%\n",
      "              AdaBoost 99 Normalized Data  96.209%  81.824% 72.349%   98.667% 98.906%\n",
      "              AdaBoost 99     SelectKBest  96.683%  85.866% 79.372%   94.818% 98.947%\n",
      "              AdaBoost 99           RFECV  93.839%  71.114% 65.767%   82.135% 94.113%\n",
      "              AdaBoost 99             PCA  94.787%  77.576% 72.964%   84.361% 96.282%\n",
      "               XGBoost 90   Original Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 90 Normalized Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 90     SelectKBest  95.261%  75.344% 67.424%   98.350% 95.664%\n",
      "               XGBoost 90           RFECV  95.261%  75.344% 67.424%   98.350% 95.664%\n",
      "               XGBoost 90             PCA  94.313%  68.221% 60.227%   98.039% 94.929%\n",
      "               XGBoost 95   Original Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 95 Normalized Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 95     SelectKBest  95.261%  75.344% 67.424%   98.350% 95.664%\n",
      "               XGBoost 95           RFECV  95.261%  75.344% 67.424%   98.350% 95.664%\n",
      "               XGBoost 95             PCA  93.839%  67.487% 64.047%   89.843% 92.103%\n",
      "               XGBoost 99   Original Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 99 Normalized Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 99     SelectKBest  95.261%  75.344% 67.424%   98.350% 95.664%\n",
      "               XGBoost 99           RFECV  95.261%  75.344% 67.424%   98.350% 95.664%\n",
      "               XGBoost 99             PCA  94.787%  72.332% 63.258%   98.194% 96.143%\n",
      "               Bagging 90   Original Data  93.365%  59.939% 53.030%   97.735% 95.490%\n",
      "               Bagging 90 Normalized Data  93.365%  59.939% 53.030%   97.735% 94.922%\n",
      "               Bagging 90     SelectKBest  94.313%  68.831% 59.091%   98.039% 93.746%\n",
      "               Bagging 90           RFECV  90.995%  31.762% 33.333%   30.332% 70.980%\n",
      "               Bagging 90             PCA  90.995%  31.762% 33.333%   30.332% 71.309%\n",
      "               Bagging 95   Original Data  93.365%  59.939% 53.030%   97.735% 96.765%\n",
      "               Bagging 95 Normalized Data  93.365%  59.939% 53.030%   97.735% 94.940%\n",
      "               Bagging 95     SelectKBest  94.313%  68.831% 59.091%   98.039% 80.976%\n",
      "               Bagging 95           RFECV  87.204%  34.747% 35.938%   33.682% 70.229%\n",
      "               Bagging 95             PCA  87.204%  34.747% 35.938%   33.682% 70.777%\n",
      "               Bagging 99   Original Data  92.891%  55.818% 48.864%   97.584% 92.739%\n",
      "               Bagging 99 Normalized Data  93.365%  59.939% 53.030%   97.735% 96.042%\n",
      "               Bagging 99     SelectKBest  94.313%  68.831% 59.091%   98.039% 88.690%\n",
      "               Bagging 99           RFECV  90.995%  31.762% 33.333%   30.332% 71.255%\n",
      "               Bagging 99             PCA  90.995%  31.762% 33.333%   30.332% 71.255%\n",
      "\n",
      "Results saved to model_results.csv\n",
      "\n",
      "====================================================================================================\n",
      "SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\n",
      "====================================================================================================\n",
      "                 ML Model   Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
      "Support Vector Machine 90             PCA  99.052%  96.494% 93.939%   99.656% 99.982%\n",
      "Support Vector Machine 95             PCA  98.578%  94.186% 89.773%   99.487% 99.644%\n",
      "Support Vector Machine 99             PCA  98.104%  93.083% 92.456%   94.097% 99.872%\n",
      "Support Vector Machine 90   Original Data  98.104%  92.171% 86.742%   99.320% 99.754%\n",
      "Support Vector Machine 90 Normalized Data  98.104%  92.171% 86.742%   99.320% 99.692%\n",
      "Support Vector Machine 90     SelectKBest  98.104%  92.171% 86.742%   99.320% 99.664%\n",
      "Support Vector Machine 90           RFECV  98.104%  92.171% 86.742%   99.320% 99.646%\n",
      "Support Vector Machine 95   Original Data  98.104%  92.171% 86.742%   99.320% 99.745%\n",
      "Support Vector Machine 95 Normalized Data  98.104%  92.171% 86.742%   99.320% 99.710%\n",
      "Support Vector Machine 95     SelectKBest  98.104%  92.171% 86.742%   99.320% 99.646%\n",
      "Support Vector Machine 95           RFECV  98.104%  92.171% 86.742%   99.320% 99.646%\n",
      "Support Vector Machine 99   Original Data  98.104%  92.171% 86.742%   99.320% 99.653%\n",
      "Support Vector Machine 99 Normalized Data  98.104%  92.171% 86.742%   99.320% 99.692%\n",
      "Support Vector Machine 99     SelectKBest  98.104%  92.171% 86.742%   99.320% 99.646%\n",
      "Support Vector Machine 99           RFECV  98.104%  92.171% 86.742%   99.320% 99.646%\n",
      "              AdaBoost 90   Original Data  96.683%  86.067% 80.508%   94.223% 99.210%\n",
      "              AdaBoost 90 Normalized Data  96.683%  86.067% 80.508%   94.223% 99.210%\n",
      "              AdaBoost 99     SelectKBest  96.683%  85.866% 79.372%   94.818% 98.947%\n",
      "              AdaBoost 95             PCA  96.683%  85.326% 79.372%   94.223% 97.055%\n",
      "     Gradient Boosting 90   Original Data  96.683%  84.837% 76.515%   98.828% 99.059%\n",
      "     Gradient Boosting 99   Original Data  96.209%  82.058% 73.485%   98.667% 99.014%\n",
      "     Gradient Boosting 99 Normalized Data  96.209%  82.058% 73.485%   98.667% 98.729%\n",
      "     Gradient Boosting 95           RFECV  96.209%  81.824% 72.349%   98.667% 96.797%\n",
      "              AdaBoost 90     SelectKBest  96.209%  81.824% 72.349%   98.667% 98.906%\n",
      "              AdaBoost 95     SelectKBest  96.209%  81.824% 72.349%   98.667% 98.906%\n",
      "              AdaBoost 99 Normalized Data  96.209%  81.824% 72.349%   98.667% 98.906%\n",
      "              AdaBoost 99   Original Data  95.735%  80.429% 72.175%   93.898% 99.098%\n",
      "     Gradient Boosting 99     SelectKBest  95.735%  80.066% 73.311%   93.898% 95.712%\n",
      "     Gradient Boosting 90 Normalized Data  95.735%  79.044% 69.318%   98.508% 98.958%\n",
      "              AdaBoost 95   Original Data  95.735%  79.044% 69.318%   98.508% 98.454%\n",
      "              AdaBoost 95 Normalized Data  95.735%  79.044% 69.318%   98.508% 98.454%\n",
      "              AdaBoost 99             PCA  94.787%  77.576% 72.964%   84.361% 96.282%\n",
      "     Gradient Boosting 90     SelectKBest  95.261%  76.927% 70.281%   93.738% 94.257%\n",
      "     Gradient Boosting 90           RFECV  95.261%  76.927% 70.281%   93.738% 95.365%\n",
      "     Gradient Boosting 95     SelectKBest  95.261%  75.906% 66.288%   98.350% 95.401%\n",
      "               XGBoost 90   Original Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 90 Normalized Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 95   Original Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 95 Normalized Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 99   Original Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 99 Normalized Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 90     SelectKBest  95.261%  75.344% 67.424%   98.350% 95.664%\n",
      "               XGBoost 90           RFECV  95.261%  75.344% 67.424%   98.350% 95.664%\n",
      "               XGBoost 95     SelectKBest  95.261%  75.344% 67.424%   98.350% 95.664%\n",
      "               XGBoost 95           RFECV  95.261%  75.344% 67.424%   98.350% 95.664%\n",
      "               XGBoost 99     SelectKBest  95.261%  75.344% 67.424%   98.350% 95.664%\n",
      "               XGBoost 99           RFECV  95.261%  75.344% 67.424%   98.350% 95.664%\n",
      "     Gradient Boosting 95   Original Data  94.787%  74.708% 66.114%   91.675% 98.637%\n",
      "              AdaBoost 90           RFECV  94.787%  74.708% 66.114%   91.675% 96.756%\n",
      "     Gradient Boosting 99           RFECV  94.787%  73.353% 67.251%   93.580% 95.682%\n",
      "         Random Forest 90     SelectKBest  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 90           RFECV  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 95     SelectKBest  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 95           RFECV  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 99     SelectKBest  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 99           RFECV  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "     Gradient Boosting 95 Normalized Data  94.787%  72.332% 63.258%   98.194% 98.371%\n",
      "               XGBoost 99             PCA  94.787%  72.332% 63.258%   98.194% 96.143%\n",
      "              AdaBoost 99           RFECV  93.839%  71.114% 65.767%   82.135% 94.113%\n",
      "              AdaBoost 95           RFECV  92.891%  69.893% 65.420%   76.086% 94.208%\n",
      "              AdaBoost 90             PCA  93.839%  69.374% 62.910%   84.287% 96.102%\n",
      "         Random Forest 90             PCA  94.313%  68.831% 59.091%   98.039% 94.508%\n",
      "         Random Forest 99             PCA  94.313%  68.831% 59.091%   98.039% 96.310%\n",
      "               Bagging 90     SelectKBest  94.313%  68.831% 59.091%   98.039% 93.746%\n",
      "               Bagging 95     SelectKBest  94.313%  68.831% 59.091%   98.039% 80.976%\n",
      "               Bagging 99     SelectKBest  94.313%  68.831% 59.091%   98.039% 88.690%\n",
      "         Random Forest 95             PCA  94.313%  68.283% 57.954%   98.039% 94.209%\n",
      "               XGBoost 90             PCA  94.313%  68.221% 60.227%   98.039% 94.929%\n",
      "               XGBoost 95             PCA  93.839%  67.487% 64.047%   89.843% 92.103%\n",
      "     Gradient Boosting 95             PCA  93.839%  64.720% 56.061%   97.886% 91.134%\n",
      "     Gradient Boosting 99             PCA  93.839%  63.438% 57.197%   97.886% 95.164%\n",
      "         Random Forest 90   Original Data  93.365%  59.939% 53.030%   97.735% 97.963%\n",
      "         Random Forest 90 Normalized Data  93.365%  59.939% 53.030%   97.735% 97.968%\n",
      "         Random Forest 95   Original Data  93.365%  59.939% 53.030%   97.735% 97.963%\n",
      "         Random Forest 95 Normalized Data  93.365%  59.939% 53.030%   97.735% 97.968%\n",
      "         Random Forest 99   Original Data  93.365%  59.939% 53.030%   97.735% 97.963%\n",
      "         Random Forest 99 Normalized Data  93.365%  59.939% 53.030%   97.735% 97.968%\n",
      "     Gradient Boosting 90             PCA  93.365%  59.939% 53.030%   97.735% 92.375%\n",
      "               Bagging 90   Original Data  93.365%  59.939% 53.030%   97.735% 95.490%\n",
      "               Bagging 90 Normalized Data  93.365%  59.939% 53.030%   97.735% 94.922%\n",
      "               Bagging 95   Original Data  93.365%  59.939% 53.030%   97.735% 96.765%\n",
      "               Bagging 95 Normalized Data  93.365%  59.939% 53.030%   97.735% 94.940%\n",
      "               Bagging 99 Normalized Data  93.365%  59.939% 53.030%   97.735% 96.042%\n",
      "               Bagging 99   Original Data  92.891%  55.818% 48.864%   97.584% 92.739%\n",
      "               Bagging 95           RFECV  87.204%  34.747% 35.938%   33.682% 70.229%\n",
      "               Bagging 95             PCA  87.204%  34.747% 35.938%   33.682% 70.777%\n",
      "               Bagging 90           RFECV  90.995%  31.762% 33.333%   30.332% 70.980%\n",
      "               Bagging 90             PCA  90.995%  31.762% 33.333%   30.332% 71.309%\n",
      "               Bagging 99           RFECV  90.995%  31.762% 33.333%   30.332% 71.255%\n",
      "               Bagging 99             PCA  90.995%  31.762% 33.333%   30.332% 71.255%\n",
      "\n",
      "Sorted results saved to sorted_model_results.csv\n",
      "\n",
      "====================================================================================================\n",
      "TOP CONFIGURATION PER MODEL\n",
      "====================================================================================================\n",
      "                 ML Model Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
      "              AdaBoost 90 Original Data  96.683%  86.067% 80.508%   94.223% 99.210%\n",
      "              AdaBoost 95           PCA  96.683%  85.326% 79.372%   94.223% 97.055%\n",
      "              AdaBoost 99   SelectKBest  96.683%  85.866% 79.372%   94.818% 98.947%\n",
      "               Bagging 90   SelectKBest  94.313%  68.831% 59.091%   98.039% 93.746%\n",
      "               Bagging 95   SelectKBest  94.313%  68.831% 59.091%   98.039% 80.976%\n",
      "               Bagging 99   SelectKBest  94.313%  68.831% 59.091%   98.039% 88.690%\n",
      "     Gradient Boosting 90 Original Data  96.683%  84.837% 76.515%   98.828% 99.059%\n",
      "     Gradient Boosting 95         RFECV  96.209%  81.824% 72.349%   98.667% 96.797%\n",
      "     Gradient Boosting 99 Original Data  96.209%  82.058% 73.485%   98.667% 99.014%\n",
      "         Random Forest 90   SelectKBest  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 95   SelectKBest  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "         Random Forest 99   SelectKBest  94.787%  72.332% 63.258%   98.194% 96.243%\n",
      "Support Vector Machine 90           PCA  99.052%  96.494% 93.939%   99.656% 99.982%\n",
      "Support Vector Machine 95           PCA  98.578%  94.186% 89.773%   99.487% 99.644%\n",
      "Support Vector Machine 99           PCA  98.104%  93.083% 92.456%   94.097% 99.872%\n",
      "               XGBoost 90 Original Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 95 Original Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "               XGBoost 99 Original Data  95.261%  75.543% 65.151%   98.350% 98.604%\n",
      "\n",
      "Top configuration per model saved to top_configurations.csv\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataframe\n",
    "result = pd.DataFrame({\n",
    "    'ML Model': ML_Model,\n",
    "    'Configuration': ML_Config,\n",
    "    'Accuracy': [f\"{acc * 100:.3f}%\" for acc in accuracy],\n",
    "    'F1 Score': [f\"{f1 * 100:.3f}%\" for f1 in f1_score],\n",
    "    'Recall': [f\"{rec * 100:.3f}%\" for rec in recall],\n",
    "    'Precision': [f\"{prec * 100:.3f}%\" for prec in precision],\n",
    "    'ROC_AUC': [f\"{roc * 100:.3f}%\" for roc in auc_roc],\n",
    "})\n",
    "\n",
    "# Remove duplicates based on model and configuration\n",
    "result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
    "\n",
    "# Display the result\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MODEL PERFORMANCE RESULTS\")\n",
    "print(\"=\" * 100)\n",
    "print(result.to_string(index=False))\n",
    "\n",
    "# Save the result to a CSV file\n",
    "result.to_csv('results/model_results.csv', index=False)\n",
    "print(\"\\nResults saved to model_results.csv\")\n",
    "\n",
    "# Sort by Accuracy and F1 Score\n",
    "sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the sorted result\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\")\n",
    "print(\"=\" * 100)\n",
    "print(sorted_result.to_string(index=False))\n",
    "\n",
    "# Save the sorted result\n",
    "sorted_result.to_csv('results/sorted_model_results.csv', index=False)\n",
    "print(\"\\nSorted results saved to sorted_model_results.csv\")\n",
    "\n",
    "# Extract top configuration per ML model\n",
    "top_per_model = sorted_result.groupby('ML Model', as_index=False).first()\n",
    "\n",
    "# Display and save the top configuration table\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"TOP CONFIGURATION PER MODEL\")\n",
    "print(\"=\" * 100)\n",
    "print(top_per_model.to_string(index=False))\n",
    "\n",
    "top_per_model.to_csv('results/top_configurations.csv', index=False)\n",
    "print(\"\\nTop configuration per model saved to top_configurations.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "720aa104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read input CSV\n",
    "df = pd.read_csv('results/top_configurations.csv')\n",
    "\n",
    "# Sort by 'Accuracy' column in descending order\n",
    "df_sorted = df.sort_values(by=['F1 Score', 'Accuracy'], ascending=False)\n",
    "\n",
    "# Save the sorted DataFrame to a new CSV\n",
    "df_sorted.to_csv('results/sorted_top_configurations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143cf9c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
